{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8b3d9f-1fb8-45f8-b762-abcab469c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "from pathlib import Path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "from dataset import get_paths, PathLike, is_image\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from dataset.path_utils import get_paths_from_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6562eb62-e360-464f-a932-f63d6bf46331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¼ Ð´Ð°Ñ‚ÐµÑÐµÑ‚Ðµ Ñ Ñ€Ð°Ð·Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¾Ð¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¸ Ð²Ñ‹Ð²ÐµÑÑ‚Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mdataset_root\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_root' is not defined"
     ]
    }
   ],
   "source": [
    "    # Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¼ Ð´Ð°Ñ‚ÐµÑÐµÑ‚Ðµ Ñ Ñ€Ð°Ð·Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ‚ÐºÐ¾Ð¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¸ Ð²Ñ‹Ð²ÐµÑÑ‚Ð¸ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸\n",
    "os.listdir(dataset_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7f05582b-1965-48b1-9981-058780d1de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2_test = Path('../../../dataset/segmentation/test/')\n",
    "\n",
    "dataset_root = \"/home/kirilman/Project/dataset/segmentation/test.yaml\"\n",
    "path_2_nets  = {'small': Path('../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt'),\n",
    "                'large': Path('../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt'),\n",
    "                'gener': Path('../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt'),\n",
    "                'generl':Path('../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt') }\n",
    "\n",
    "path_2_dataset = {'small': Path('../../../dataset/segmentation/labels/small/test/'),\n",
    "                  'large': Path('../../../dataset/segmentation/labels/large_segment/test/'),\n",
    "                  'gener': Path('../../../dataset/segmentation/labels/original/test/'),}\n",
    "\n",
    "def copy(_from, to):\n",
    "    files = os.listdir(_from)\n",
    "    files = list(filter( lambda x:False if is_image(x) else True, files))\n",
    "    for file in files:\n",
    "        shutil.copy(_from / file, to / file)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8ff04ea-0f12-442b-8cb3-46735f4163e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small\n",
      "large\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "for name, path_2_data in path_2_dataset.items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1cbeb70-cf98-4573-b519-fac5c73afcf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:* small * ../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=small, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test' images and labe\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/kirilman/Project/dataset/segmentation/test.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1278      0.618      0.546      0.575      0.355      0.615      0.544      0.569      0.331\n",
      "                asbest         44       1278      0.618      0.546      0.575      0.355      0.615      0.544      0.569      0.331\n",
      "Speed: 1.3ms pre-process, 22.9ms inference, 1.2ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/small7\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* small * ../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=large, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1278      0.411      0.183       0.17      0.105      0.405      0.181      0.165      0.101\n",
      "                asbest         44       1278      0.411      0.183       0.17      0.105      0.405      0.181      0.165      0.101\n",
      "Speed: 1.1ms pre-process, 22.1ms inference, 0.8ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/large7\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* small * ../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=gener, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 220 layers, 21656399 parameters, 0 gradients, 69.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1278      0.536      0.593      0.507       0.31      0.536      0.591      0.504      0.285\n",
      "                asbest         44       1278      0.536      0.593      0.507       0.31      0.536      0.591      0.504      0.285\n",
      "Speed: 1.3ms pre-process, 13.3ms inference, 1.7ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/gener7\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* small * ../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=generl, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1278      0.567      0.629       0.51      0.314      0.562      0.624      0.501      0.291\n",
      "                asbest         44       1278      0.567      0.629       0.51      0.314      0.562      0.624      0.501      0.291\n",
      "Speed: 1.4ms pre-process, 24.5ms inference, 1.6ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/generl\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* large * ../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=small, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test' images and labe\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/kirilman/Project/dataset/segmentation/test.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44        602      0.118      0.181     0.0739     0.0499      0.118      0.181     0.0737     0.0488\n",
      "                asbest         44        602      0.118      0.181     0.0739     0.0499      0.118      0.181     0.0737     0.0488\n",
      "Speed: 0.7ms pre-process, 22.2ms inference, 1.0ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/small8\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* large * ../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=large, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44        602      0.671      0.591      0.579      0.389      0.665       0.59      0.572      0.379\n",
      "                asbest         44        602      0.671      0.591      0.579      0.389      0.665       0.59      0.572      0.379\n",
      "Speed: 0.8ms pre-process, 22.3ms inference, 0.7ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/large8\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* large * ../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=gener, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 220 layers, 21656399 parameters, 0 gradients, 69.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44        602      0.392      0.487       0.35      0.232      0.398      0.497      0.355      0.232\n",
      "                asbest         44        602      0.392      0.487       0.35      0.232      0.398      0.497      0.355      0.232\n",
      "Speed: 0.9ms pre-process, 14.6ms inference, 1.8ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/gener8\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* large * ../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=generl, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44        602       0.44      0.525      0.416       0.29      0.437      0.521      0.415      0.284\n",
      "                asbest         44        602       0.44      0.525      0.416       0.29      0.437      0.521      0.415      0.284\n",
      "Speed: 0.7ms pre-process, 21.2ms inference, 1.5ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/generl2\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* gener * ../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/small_l_b4_512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=small, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test' images and labe\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/kirilman/Project/dataset/segmentation/test.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1600      0.569      0.477       0.49      0.303      0.594      0.454      0.483      0.282\n",
      "                asbest         44       1600      0.569      0.477       0.49      0.303      0.594      0.454      0.483      0.282\n",
      "Speed: 1.4ms pre-process, 22.3ms inference, 1.1ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/small9\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* gener * ../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/large_l_b4_512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=large, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1600      0.773      0.273      0.367      0.241      0.764      0.271       0.36      0.232\n",
      "                asbest         44       1600      0.773      0.273      0.367      0.241      0.764      0.271       0.36      0.232\n",
      "Speed: 1.6ms pre-process, 27.2ms inference, 1.3ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/large9\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* gener * ../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/m_batch4_imgs512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=gener, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 220 layers, 21656399 parameters, 0 gradients, 69.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1600      0.648      0.594      0.636      0.398       0.66      0.586      0.634      0.373\n",
      "                asbest         44       1600      0.648      0.594      0.636      0.398       0.66      0.586      0.634      0.373\n",
      "Speed: 1.6ms pre-process, 14.8ms inference, 1.8ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/gener9\u001b[0m\n",
      "-------------------------------------------\n",
      "dataset:* gener * ../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt *\n",
      "\u001b[34m\u001b[1msegment/val: \u001b[0mdata=/home/kirilman/Project/dataset/segmentation/test.yaml, weights=['../../../asbestos/yolov5/runs/train-seg/l_batch4_imgs512/weights/best.pt'], batch_size=1, imgsz=512, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=./result/, name=generl, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v6.2-216-g6e544d5f Python-3.9.12 torch-1.11.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5947MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 275 layers, 47474367 parameters, 0 gradients, 146.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/kirilman/Project/dataset/segmentation/test.cache' images an\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         44       1600      0.713      0.616      0.659       0.42        0.7      0.613       0.65      0.394\n",
      "                asbest         44       1600      0.713      0.616      0.659       0.42        0.7      0.613       0.65      0.394\n",
      "Speed: 1.5ms pre-process, 22.9ms inference, 1.6ms NMS per image at shape (1, 3, 512, 512)\n",
      "Results saved to \u001b[1mresult/generl3\u001b[0m\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n, path_2_data in path_2_dataset.items():\n",
    "    copy(path_2_data, path_2_test)\n",
    "    for name, path_2_net in path_2_nets.items():\n",
    "        print('dataset:*',n,'*', path_2_net,'*')\n",
    "        !python ../../yolov5/segment/val.py --data {dataset_root} --weight {path_2_net} --project ./result/ --imgsz 512 --batch-size 1 --name {name} > r.txt\n",
    "        print('-------------------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b4d92568-6fc6-4c81-8895-e4f21a562723",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('r.txt','r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6461874d-5325-490f-95da-57aa1ca0a9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for l in data.split('\\n'):\n",
    "    if l[:3] == 'dat':\n",
    "        dataset = l.split('*')[1]\n",
    "        name_net = l.split('*')[2]\n",
    "    if 'asbest' in l:\n",
    "        line_res = l\n",
    "    if l[:3] == '---':\n",
    "        r = dataset+'|'+name_net+'|'+line_res\n",
    "        t =list(filter(lambda x: True if len(x)>1 else False, r.split(' ')))\n",
    "        r = {'dataset':t[0],'net':t[1],'mAP50':t[-2], 'mAP50-95':t[-1]}\n",
    "        result.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c9874c0a-df06-4bc3-86bc-136d0777fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    r['net'] = r['net'].split('/')[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8e62eb2d-93b5-4db7-9f07-69adea6ae128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>net</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>small</td>\n",
       "      <td>small_l_b4_512</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>small</td>\n",
       "      <td>large_l_b4_512</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small</td>\n",
       "      <td>m_batch4_imgs512</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small</td>\n",
       "      <td>l_batch4_imgs512</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>large</td>\n",
       "      <td>small_l_b4_512</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.0504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>large</td>\n",
       "      <td>large_l_b4_512</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>large</td>\n",
       "      <td>m_batch4_imgs512</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>large</td>\n",
       "      <td>l_batch4_imgs512</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gener</td>\n",
       "      <td>small_l_b4_512</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gener</td>\n",
       "      <td>large_l_b4_512</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gener</td>\n",
       "      <td>m_batch4_imgs512</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gener</td>\n",
       "      <td>l_batch4_imgs512</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset               net  mAP50 mAP50-95\n",
       "0    small    small_l_b4_512  0.576    0.335\n",
       "1    small    large_l_b4_512  0.191    0.115\n",
       "2    small  m_batch4_imgs512  0.516     0.29\n",
       "3    small  l_batch4_imgs512  0.511    0.294\n",
       "4    large    small_l_b4_512  0.076   0.0504\n",
       "5    large    large_l_b4_512  0.587    0.388\n",
       "6    large  m_batch4_imgs512  0.357    0.233\n",
       "7    large  l_batch4_imgs512  0.416    0.284\n",
       "8    gener    small_l_b4_512  0.492    0.287\n",
       "9    gener    large_l_b4_512  0.394    0.253\n",
       "10   gener  m_batch4_imgs512  0.647    0.378\n",
       "11   gener  l_batch4_imgs512  0.659    0.398"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "de8e84de-8ad2-48c7-a378-1dbda11d9b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "with open('t.txt','w') as f:\n",
    "    p = subprocess.Popen(['python',\"run.py\"],stdout = f)\n",
    "    p.wait()\n",
    "    print(p.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ef4532e1-bf8c-4eb5-9db9-f9e87d18a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kirilman/Project/asbestos/notebooks/visualize_result\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616033f-f2e3-4230-8dd2-70657e767738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = get_paths_from_dirs(['../../yolov5/runs/train-seg/'],['pt'])\n",
    "models = list(filter(lambda x: x.name.split('/')[-1] == 'best.pt', models))\n",
    "print(len(models), models[-1])\n",
    "path_test_yml = Path('../../../dataset/segmentation/test.yaml')\n",
    "copy(Path('../../../dataset/segmentation/labels/original/test/'), Path('../../../dataset/segmentation/test/'))\n",
    "for path_2_model in models:\n",
    "    name = str(models[0]).split('/')[-3]\n",
    "    !python ../../yolov5/segment/val.py --data {path_test_yml} --weight {path_2_model} --imgsz 512 --batch-size 1 --name {name} > r.txt\n",
    "    print('-----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "03adb26a-e70c-4c31-af29-ca03f0466459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>large_l_b4_512</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x_512_batch_1@</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp2</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l_batch4_imgs512</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exp</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>merge_416_b2</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.0786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>m_batch4_imgs512</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>l_512_batch_2</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>m_512_b4@</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>exp3</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>small_l_b4_512</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>x_batch4_imgs512</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 net      P      R  mAP50 mAP50-95\n",
       "0     large_l_b4_512  0.762  0.271   0.36    0.232\n",
       "1     x_512_batch_1@  0.654  0.616  0.628    0.383\n",
       "2               exp2  0.566  0.631  0.598    0.281\n",
       "3   l_batch4_imgs512  0.706  0.613   0.65    0.394\n",
       "4                exp  0.442  0.438  0.403    0.158\n",
       "5       merge_416_b2  0.389  0.388  0.251   0.0786\n",
       "6   m_batch4_imgs512   0.66  0.586  0.634    0.373\n",
       "7      l_512_batch_2  0.682  0.682  0.703    0.436\n",
       "8          m_512_b4@  0.651   0.62  0.619    0.375\n",
       "9               exp3  0.675  0.631  0.695    0.382\n",
       "10    small_l_b4_512  0.594  0.454  0.483    0.282\n",
       "11  x_batch4_imgs512  0.689  0.627  0.684    0.411"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "import re\n",
    "with open('r.txt','r') as f:\n",
    "    data = f.read()\n",
    "for l in data.split('\\n'):\n",
    "    if l[:11] == 'segment/val':\n",
    "        a = re.search(\"\\[.\",l)\n",
    "        a = a.start() + 2\n",
    "        b = re.search(\"\\]\",l)\n",
    "        b = b.start() - 1\n",
    "        model = l[a:b].split('/')[-3]\n",
    "    if 'asbest' in l:\n",
    "        t =list(filter(lambda x: True if len(x)>1 else False, l.split(' ')))\n",
    "        r = { 'net':model,'P':t[-4], 'R':t[-3], 'mAP50':t[-2], 'mAP50-95':t[-1]}\n",
    "        result.append(r)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c548efa3-c1da-4a13-8f97-3dd062b5baad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m_batch4_imgs512</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x_batch4_imgs512</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small_l_b4_512</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x_b8_img640</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>large_l_b4_512</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all_l_b4_512</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>merge_l_b4_512</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>merge_filter_small_l_b4_512</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>merge_filter_small_l_b4_512_flip</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>merge_filter_small_l_b4_512_flip2</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 net      P       R  mAP50 mAP50-95\n",
       "0                   m_batch4_imgs512  0.704  0.0774  0.386    0.279\n",
       "1                   x_batch4_imgs512  0.601   0.158  0.395    0.279\n",
       "2                     small_l_b4_512  0.662   0.156  0.408    0.272\n",
       "3                        x_b8_img640  0.776   0.181  0.468    0.339\n",
       "4                     large_l_b4_512  0.797    0.26  0.515    0.376\n",
       "5                       all_l_b4_512  0.733  0.0876  0.407    0.299\n",
       "6                     merge_l_b4_512  0.672   0.639  0.652     0.38\n",
       "7        merge_filter_small_l_b4_512  0.703   0.601  0.668    0.404\n",
       "8   merge_filter_small_l_b4_512_flip  0.703   0.601  0.668    0.404\n",
       "9  merge_filter_small_l_b4_512_flip2  0.704   0.629  0.708    0.424"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "import re\n",
    "with open('result_val.txt','r') as f:\n",
    "    data = f.read()\n",
    "for l in data.split('\\n'):\n",
    "    if l[:11] == 'segment/val':\n",
    "        a = re.search(\"\\[.\",l)\n",
    "        a = a.start() + 2\n",
    "        b = re.search(\"\\]\",l)\n",
    "        b = b.start() - 1\n",
    "        model = l[a:b].split('/')[-3]\n",
    "    if 'rocks' in l:\n",
    "        t =list(filter(lambda x: True if len(x)>1 else False, l.split(' ')))\n",
    "        r = { 'net':model,'P':t[-4], 'R':t[-3], 'mAP50':t[-2], 'mAP50-95':t[-1]}\n",
    "        result.append(r)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb10dbb5-595e-4003-b2a7-88fd0ff26995",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>net</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>mAP50</th>\n",
       "      <th>mAP50-95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m_batch4_imgs512</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x_batch4_imgs512</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small_l_b4_512</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x_b8_img640</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>large_l_b4_512</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all_l_b4_512</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>merge_l_b4_512</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>merge_filter_small_l_b4_512</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>merge_filter_small_l_b4_512_flip</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>merge_filter_small_l_b4_512_flip2</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 net      P       R  mAP50 mAP50-95\n",
       "0                   m_batch4_imgs512  0.662   0.585  0.634    0.373\n",
       "1                   x_batch4_imgs512  0.687   0.628  0.684    0.412\n",
       "2                     small_l_b4_512  0.595   0.454  0.483    0.282\n",
       "3                        x_b8_img640  0.712   0.622  0.654      0.4\n",
       "4                     large_l_b4_512  0.764   0.271   0.36    0.232\n",
       "5                       all_l_b4_512  0.701   0.613  0.649    0.394\n",
       "6                     merge_l_b4_512  0.606  0.0981  0.346    0.225\n",
       "7        merge_filter_small_l_b4_512  0.737   0.212  0.475    0.337\n",
       "8   merge_filter_small_l_b4_512_flip  0.737   0.212  0.475    0.337\n",
       "9  merge_filter_small_l_b4_512_flip2  0.563   0.111  0.341    0.244"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "import re\n",
    "with open('./result/result_val_origin.txt','r') as f:\n",
    "    data = f.read()\n",
    "for l in data.split('\\n'):\n",
    "    if l[:11] == 'segment/val':\n",
    "        a = re.search(\"\\[.\",l)\n",
    "        a = a.start() + 2\n",
    "        b = re.search(\"\\]\",l)\n",
    "        b = b.start() - 1\n",
    "        model = l[a:b].split('/')[-3]\n",
    "    if ' all' in l:\n",
    "        t =list(filter(lambda x: True if len(x)>1 else False, l.split(' ')))\n",
    "        r = { 'net':model,'P':t[-4], 'R':t[-3], 'mAP50':t[-2], 'mAP50-95':t[-1]}\n",
    "        result.append(r)\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d769d99-b10f-4759-9099-5ae60ecbee52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
